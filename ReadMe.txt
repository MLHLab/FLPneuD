In the realm of medical image analysis, federated learning (FL) has ushered in a transformative era in healthcare AI, allowing collaborative model training across diverse datasets without centralizing sensitive patient information. This research focuses on pioneering a FL framework tailored for pneumonia classification through chest X-ray images. A centralized FL topology was crafted, featuring three client nodes and one server node. A novel deep convolutional neural network (CNN)-based architecture served as the backbone classification module in each node. Utilizing data from the comprehensive repository available at https://github.com/ieee8023/covid-chestxray-dataset, two distinct experimental setups were devised. Setup A delineated viral versus non-viral data, while setup B specifically categorized covid versus non-covid cases within the viral disease dataset. In a comparative evaluation with standard image classification networks like ResNet50, VGG16, DenseNet, and Inception v3, the proposed deep CNN-based model displayed comparable performance. When manifested into the server-client architecture, the central FL model exhibited remarkable performance metrics. These outstanding scores underscore the model's ability to accurately classify disease categories, all while maintaining stringent privacy protocols and data security, establishing it as an effective and reliable diagnostic framework.

As illustrated in the following figure, in the centralized FL process, several pivotal steps contribute to the cohesive orchestration of model optimization across distributed client sites. First, the initiation of learning iterations takes place, instigated by the controlling server with the aim of refining and optimizing the model. Subsequently, the assessment of client suitability occurs, wherein the server appraises each client's condition to ascertain its eligibility for participation in the training procedure. This evaluation considers factors such as accessibility, system occupancy, and environmental variables. Upon selection, chosen clients are endowed with the current model weights from the server, while retaining their individualized training modules. Following this, each client undertakes autonomous training of its local model utilizing available local data and training algorithms, possibly employing techniques like Stochastic Gradient Descent (SGD) similar to Federated Averaging. Upon completion, clients transmit their model updates back to the server, which then integrates these updates to revise the central model. Subsequently, the central server communicates final updates to connected clients, enabling them to adjust their respective models accordingly. Post successful local model update, clients utilize these refined models for subsequent local predictions. This process collectively ensures collaborative model improvement within a centralized framework. Each of these stages collectively constitutes the orchestrated process of centralized FL, enabling the optimization of models across distributed client sites.

 
Figure. Work-flow of the centralized FL architecture

ReadMe file with code description:
1. FL_Client		-->	Federated learning client side model 
2. FL_Server		-->	Federated learning server side model
3. Dataload		-->	Github data download
4. PictureSorter	-->	Sorts Pictures to folders
5. DenseNet		-->	Densenet model
6. ResNet50		-->	ResNet50 model
7. VGG16		-->	VGG16 model
8. InceptionV3		-->	InceptionV3 model
